---
title: A deep dive into variational auto-encoder
layout: post
categories: "machine learning"
permalink: vae.html
excerpt: An in-depth notes on VAE, with pytorch step-by-step explanation
---

This is a deep dive to maching learning technique *variational auto-encoder* (VAE).


* TOC
{:toc}

## Part 1 - General Theory
### Model
The digit images ($$x$$) are generated by an unknown process $$p_{\theta^*}(x\vert z)$$, where $$z$$ is an unobservable latent variable. Here we plug in a specific $$z$$ into this function, and it specifies the intensity *distribution* of the image pixels of $$x$$. 


We don't know the true parameter $$\theta^*$$ but we know the general form of the parametric function $$p_{\theta}(x\vert z)$$. Note that $$\theta$$ denotes variable of parameters, while $$\theta^*$$ denotes the truth value. Latent variable $$z$$ follows distribution $$p_{\theta^*}(z)$$. 

The *likelihood function* $$p_{\theta}(x)$$ measures how well $$\theta$$ describes the observed digit $$x$$.  By definition $$p_{\theta^*}(x) = 1$$ because $$\theta^*$$ generates the observation. However, $$p_{\theta}(x)$$ is intractable, meaning that we cannot evaluate or differentiate $$p_{\theta}(x)$$ for every $$\theta$$ and $$x$$.  Same for $$p_{\theta}(x\vert z)$$ and $$p_{\theta}(z\vert x)$$.  Since $$p_{\theta}(z\vert x)$$ is hard to evaluate,  we introduce a new function $$q_{\phi}(z\vert x)$$ to approximate it.  The following diagram sums it all up:


{% mermaid %}
graph LR
  P("Draw from<br>distribution $P(z)$") --> A
  A(("latent<br>variable $z$")) --> H(Hidden<br>process )
  H -->X((observed<br>image $x$))
  X-->Q("$q(z\vert X)$")
  Q-->P
  style H fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5, 5
{% endmermaid %}

In summary, below are what each variable means.

|Variable | Description   | 
|---------|--------------|
| $$x$$     | image of digit        | 
| $$z$$     | latent variable that generates the image        | 
| $$\theta$$     | Parameter of the true model  $$p_{\theta}$$       | 
|$$\theta^*$$  |  True parameter. $$p_{\theta^*}(x)=1$$ is maximum|
|$$p_{\theta}(x)$$  |  Likelihood function - How likely $$\theta$$ describes $$x$$|
|$$p_{\theta}(z)$$  |  Prior distribution of $$z$$|
|$$p_{\theta}(x\vert z)$$  |  Likelihood or generative function of $$x$$ given $$z$$|
|$$p_{\theta}(z\vert x)$$  |  Likelihood or generative function of $$z$$ given $$x$$|
|$$q_{\phi}(z\vert x)$$  |  Approximate function to $$p_{\theta}(z\vert x)$$ |

### Objective function
In variational auto encoder, the objective function is the likelihood function of the true model $$p_{\theta}(x)$$.  Ideally we want to find the optimal $$\theta$$ that best matches the observed images $$x$$.  $$p_{\theta}(x)$$ can be expressed in terms of Kullback-Leiber divergence $$D_{KL}$$.  $$D_{KL}(q_{\phi}(z\vert x) ||  p_{\theta(z\vert x) })$$ measures how well $$q_{\phi}(z\vert x)$$ approximates  $$p_{\theta(z\vert x)}$$:

$$D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta(z\vert x) })  = \mathbb{E}_{q_{\phi}(z\vert x)}[\ln q_{\phi}(z\vert x)-\ln p(x,z) + \ln p_{\theta}(x)]$$

or

$$
\ln p_{\theta}(x) = D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta}(z\vert x) )  + \mathcal{L}.
$$ 

where 

$$
\mathcal{L} = \mathbb{E}_{q_{\phi}(z\vert x)}[-\ln q_{\phi}(z\vert x)+\ln p_{\theta}(x,z) ]
$$

$$\ln p_{\theta}(x)$$ can be taken out of the expectation because it does not depend on $$z$$. Since D_{KL} is non-negative, $$\mathcal{L}$$ serves as a *lower bound* to $$\ln p_{\theta}(x)$$. In other words:

$$\ln p_{\theta}(x)  \ge \mathcal{L}$$


The trick of VAE is to  maximize $$\mathcal{L}$$ instead of $$\ln p_{\theta}(x)$$ because it can be calculated for many problems.

But since $$p(x,y)$$ is hard to calculate, it is useful to rewrite $$\mathcal{L}$$ as 

$$
\begin{array}
\mathcal{L} &= \mathbb{E}_{q_{\phi}(z\vert x)}[-\ln q_{\phi}(z\vert x)+\ln p_{\theta}(x,z) ]\\
            &= \mathbb{E}_{q_{\phi}(z\vert x)}[-\ln q_{\phi}(z\vert x)+\ln p_{\theta}(z)+\ln p_{\theta}(x|z) ]\\
            &= -D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta } (z)) + \mathbb{E}_{q_{\phi}(z\vert x)} [\ln p_{\theta}(x|z)]
\end{array}
$$



## Part 2 - Model for MNIST dataset
To trian the model, we will maximize the lower bound

$$
\mathcal{L} = -D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta } (z)) + \mathbb{E}_{q_{\phi}(z\vert x)} [\ln p_{\theta}(x|z)]
$$

For this problem, we choose the prior distribution $$p_{\theta}(z)$$ of the latent variable $$z$$ to be the standard normal distribution which has zero mean and unit variance, i.e. $$\mathcal{N}(0,1)$$. Why can we do that? Because we don't know the distribution and may as well choose to work with a easier one! But we will see in a moment that for the purpose it will serve it doesn't really matter.

OK, that takes care of $$p_{\theta}(z)$$. How about $$q_{\phi}(z\vert x)$$? To be consistent, we also model it with a normalize distribution, but it could have non-zero mean and non-unity variance. Mathematically, 

$$
q_{\phi}(z\vert x) = \mathcal{N}(\mu, \sigma^2)
$$

How can it be different from the prior? The idea is, we have 10 digits to encode. Each one will have distribution deviates from zero and collectively they form 10 distinct clusters. But if we look at $z$ over all digits, it will still follow the standard normal distribution $$\mathcal{N}(0,1)$$. In a moment, you will see the prior $$p_{\theta}(z)$$ regularizes the learned parameters $$\mu$$ and $$\sigma$$  to pull them back to the standard normal $$\mathcal{N}(0,1)$$.

Now we can calculate the first term, $$ D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta } (z)) $$. Using the [identity](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions) of KL divergence between two normal distributions

$$
D_{KL}(\mathcal{N}(\mu_1, \sigma_1^2)\vert\vert \mathcal{N}(\mu_2, \sigma_2^2)) =  \ln \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2 \sigma_2^2} - \frac{1}{2}
$$

We get the first term

$$
\begin{array}
--D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta } (z)) &= -D_{KL}(\mathcal{N}(\mu, \sigma^2) \vert \vert  \mathcal{N}(0, 1) )\\
                       &= \frac{1}{2} \sum_\limits{j=1}^J(1 + \ln \sigma_j^2 -\sigma_j^2 - \mu_j^2)
\end{array}
$$

$J$ is the dimension of the latent variable $z$.  

Let's build some intuition!
* $\ln \sigma_j^2 -\sigma_j^2$ is maximized at $\sigma_j^2 = 1$ ([plot](https://www.wolframalpha.com/input/?i=plot+ln+%28sigma%5E2%29+-+sigma%5E2+from+0+to+5))
* $-\mu_j^2$ is maximized when $\mu_j = 0$ (hope this is obvious...)

So the learning prefers $z$ to follow the standard normal distribution as much as possible. In other words, the first term is a regularization term to make sure that the learned distribution of $z$ is not too crazy. 


OK, let's understand the second term $$\mathbb{E}_{q_{\phi}(z\vert x)} [\ln p_{\theta}(x\vert z)]$$. For binary images (we will use binarized MNIST),  $p_{\theta}(x\vert z)$ is Bernoulli distribution . For a single pixel, it is

$$
p_{\theta}(x|z) =x^y(1-x)^{(1-y)}
$$

$y$ is the observed pixel intensity and can only be 0 or 1 (binary).  Basically, this is a measure of how well the random variable $x$ matches the observed intensity. Suppose it is a dark pixel ($y=0$) and the model predicts $x=0.1$,  the model would be considered to be doing pretty well and score 0.9 (with 1.0 being the full mark). Note that you don't see $z$ on right hand side of the equation because it is  implicitly in $x$ as $x$ is generated by $z$.   For an image with $L$ pixels, 

$$
p_{\theta}(x|z) =\prod\limits_{i=1}^{L} x_i^{y_i}(1-x_i)^{(1-y_i)}
$$

How to evaluate the expectation value $$\mathbb{E}_{q_{\phi}(z\vert x)}$$?  Since we draw $z$ from the approximate $q_{\phi}(z\vert x)$, we can take an simple average over all images, or use only one image per evaluation and don't worry about it.


## Part 3 - Pytorch implementation

`model` takes latent variable $$z$$  and generates digit $$x$$
```python
class Model(nn.Module):
  """Bernoulli model parameterized by a generative network with Gaussian latents for MNIST."""
  def __init__(self, latent_size, data_size):
    super().__init__()
    self.register_buffer('p_z_loc', torch.zeros(latent_size))
    self.register_buffer('p_z_scale', torch.ones(latent_size))
    self.log_p_z = NormalLogProb()
    self.log_p_x = BernoulliLogProb()
    self.generative_network = NeuralNetwork(input_size=latent_size,
                                            output_size=data_size, 
                                            hidden_size=latent_size * 2)
```
$$x$$: digit image (size: 28x28 = 784)
$$z$$: latent variable (size: 128)
`log_p_z`:  $$\log p(z)$$ is log normal distribution with zero mean and unit variance.\
`log_p_x`: $$\log p(x)$$ is log Bernoulli distribution with logits, i.e. $$y\log \sigma(x) + (1-y)\log[1-\sigma(x)]$$, $$\sigma$$ is the Sigmoid function. $$y$$ is probability of a bright pixel.
`generative_network` takes  latent variable $$z$$ and generates a digit image $$x$$. Note that need to pass x through a Sigmoid function (i.e. $$\sigma(x)$$ ) to form a digit with intensity range 0 to 1. 

   ### Feed forward of Model
```python
  def forward(self, z, x):
    """Return log probability of model.
       Generates logits from z, then compare with truth x
    """
    log_p_z = self.log_p_z(self.p_z_loc, self.p_z_scale, z).sum(-1, keepdim=True)
    logits = self.generative_network(z)
    # unsqueeze sample dimension
    logits, x = torch.broadcast_tensors(logits, x.unsqueeze(1))
    log_p_x = self.log_p_x(logits, x).sum(-1, keepdim=True) # note that this is function of both generated logits and truth x
    return log_p_z + log_p_x
```
$$\log p(z)$$ is the prior probability of observing $$z$$.  It already sums over the dimension of $$z$$.  We can do that because $$p(z) = p(z_1)p(z_2)...p(z_n)$$ for normal distribution.  So we have one value for each z.  
`logits`  $$x_z$$ is generated by the model using $$z$$. It is the generated digit image.

$$\log p( \sigma(x_z), x )$$ is function that compare $$\sigma(x_z)$$ and $$x$$.

* $$p(0,0) = 0$$
* $$p(1,1) = 0$$
* $$p(-1 ,1) = -27.6$$
* $$p(1 ,-1) = -27.6$$

for each digit.  So $$\log p( \sigma(x_z), x )$$ is maximized when the generated digit matches the truth. Note `log_p_x` is  $$\log p( \sigma(x_z), x )$$ with negative sign included.

The `forward` function returns $$p_{xz}(x,z)$$, the joint prior probability of $$x$$ and $$z$$.  Using $$\log p_{xz}(x,z) = \log p_x(x) p_z(z) = \log p_x(x) + \log p_z(z)$$, the `forward` function returns $$\log p_z(z) + \log p( \sigma(x_z),x )$$.  It has dimension of the batch size (128).
## Variational class

`VariationalMeanField` takes an image $$x$$ and returns the latent variable $$z$$

```python
class VariationalMeanField(nn.Module):
  """Approximate posterior parameterized by an inference network."""
  def __init__(self, latent_size, data_size):
    super().__init__()
    self.inference_network = NeuralNetwork(input_size=data_size, 
                                           output_size=latent_size * 2, 
                                           hidden_size=latent_size*2)
    self.log_q_z = NormalLogProb()
    self.softplus = nn.Softplus()
```

[Softplus](https://pytorch.org/docs/stable/nn.html#softplus) is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.


```python
  def forward(self, x, n_samples=1):
    """Return sample of latent variable and log prob."""
    loc, scale_arg = torch.chunk(self.inference_network(x).unsqueeze(1), chunks=2, dim=-1) # Infer parameters of z from x (loc and scale)
    scale = self.softplus(scale_arg) # make sure scale is positive
    eps = torch.randn((loc.shape[0], n_samples, loc.shape[-1]), device=loc.device)
    z = loc + scale * eps  # reparameterization
    log_q_z = self.log_q_z(loc, scale, z).sum(-1, keepdim=True)
    return z, log_q_z
```
`inference_network` returns a vector two times of size of latent variable.  It takes a digit image $$x$$ as input and output the *parameters* of probability distribution of latent variable $$z$$. i.e. mean (`loc`) and variance (`scale`)  which have dimension of latent variable $$z$$.

The reparameterization trick is to redraw the latent variable $$z$$ from normal distribution with parameters (i.e. `loc` and `scale`) inferred from the original $$z$$.  This has effect of increasing the variance of $$z$$.

$$\log q(z)$$ (`log_q_z`) is the log probability of the new $$z$$, one value per image.

### Steps

```python
  for step, batch in enumerate(cycle(train_data)):
    x = batch[0].to(device)
    model.zero_grad()
    variational.zero_grad()
    z, log_q_z = variational(x, n_samples=1)
    log_p_x_and_z = model(z, x)
    # average over sample dimension
    elbo = (log_p_x_and_z - log_q_z).mean(1)
    # sum over batch dimension
    loss = -elbo.sum(0)
    loss.backward()
    optimizer.step()
```

 1. Infer latent variable $$z$$ from $$x$$. Return the reparameterized $$z$$ and its log probability $$\log q(z)$$
 2.  Generative model (`model`)  uses reparameterized $$z$$  to get  $$\log p(z) - \log p( \sigma(x_z), x )$$ (`log_p_x_and_z`)
 3. ELBO = $$\log p(z) +\log p( \sigma(x_z),x ) - \log q(z)$$

## References
 
 [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) - original paper
 
 